{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAIN IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train.csv as the main dataset\n",
    "data = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# Column Transformation to lowercase and underscored spaces\n",
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "data.columns = data.columns.str.replace('-', '_')\n",
    "data.columns = data.columns.str.lower()\n",
    "\n",
    "X = data.loc[:, data.columns != 'lead']\n",
    "y = data.loc[:, data.columns == 'lead']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPLITTING DATA\n",
    "\n",
    "We split the dataset in to train: 75% and test: 25% using the default **train_test_split** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(779, 13), (260, 13), (779, 1), (260, 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)\n",
    "[X_train.shape, X_test.shape, y_train.shape, y_test.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET ALL FEATURE COMBINATIONS\n",
    "\n",
    "In this section, we create a function to produce sets of all possible feature combinations and save them in an array to be used in the model iteration. There will be at most $2^{8} = 8192$ (including the empty set) feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to produce an array of all feature combinations\n",
    "def get_all_feature_combinations(data_columns):\n",
    "    from itertools import chain, combinations\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    feature_combinations = list(chain.from_iterable(combinations(data_columns, r) for r in range(len(data_columns)+1)))\n",
    "\n",
    "    feature_combinations_set = []\n",
    "    for feature_combination in feature_combinations:\n",
    "        feature_combination_set = []\n",
    "        for feature in feature_combination:\n",
    "            feature_combination_set.append(feature)\n",
    "        \n",
    "        feature_combinations_set.append(feature_combination_set)\n",
    "\n",
    "    return feature_combinations_set\n",
    "\n",
    "feature_combinations = get_all_feature_combinations(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETER TUNING FUNCTION\n",
    "\n",
    "In this section, we create a function to find the best K value we could get by iterating thorugh given number of _k_iterations_. The input to this function will be training data **X** and **y** labels.\n",
    "\n",
    "The function will then iterate through _k_iterations_ which takes the data through a **GridSearchCV** pipeline which first scales the training data using **SandardScaler** and then fits a **KNeighborsClassifier** model to provide us the best K value along with it's accuracy.\n",
    "\n",
    "Here, the **GridSearchCV** pipeline handles the cross validation search within itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# A function to produce an array with best K value along with it's accuracy --> eg: returns [K = 10, 0.93]\n",
    "def find_best_k_with_accuracy_cv(X, y, k_iterations, n_fold = 10):\n",
    "\n",
    "    #knn = KNeighborsClassifier()\n",
    "    k_range = list(range(1, k_iterations + 1))\n",
    "    \n",
    "    param_grid = {\n",
    "        'knn__n_neighbors': k_range\n",
    "    }\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            ('scaler', StandardScaler()), \n",
    "            ('knn', KNeighborsClassifier(n_neighbors = k_range))\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(pipe, param_grid, cv = n_fold, scoring = 'accuracy', return_train_score = False, verbose=1)\n",
    "    \n",
    "    # fitting the model for grid search\n",
    "    grid_search=grid.fit(X, y.to_numpy().reshape(-1, ))\n",
    "\n",
    "    best_k = grid_search.best_params_.get('knn__n_neighbors')\n",
    "    accuracy = grid_search.best_score_\n",
    "\n",
    "    return [best_k, accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL ITERATOR (PARAMETER TUNING) FUNCTION\n",
    "\n",
    "In this section, we have the code to produce a model performance report for each feature combination. This should ideally be run thorugh all feature combinations (i.e. $2^{8} - 1 = 8191$ excluding the null set), and for each of the feature combination we run the above function **find_best_k_with_accuracy_cv** to give us the best K value along with it's accuracy. For computational convinience, we will be using all feature combinations which includes *at least 9* features for our model iteration.\n",
    "\n",
    "This finally produces a report in csv format, which later can be used as an input for comparing how well each of the K-NN models would perform with an unseen test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of iterations - max 8191\n",
    "iterations = 1\n",
    "\n",
    "# Setting column names for iteration results\n",
    "results_column_names = [\n",
    "        'number_words_female',\n",
    "        'total_words',\n",
    "        'number_of_words_lead',\n",
    "        'difference_in_words_lead_and_co_lead',\n",
    "        'number_of_male_actors',\n",
    "        'year',\n",
    "        'number_of_female_actors',\n",
    "        'number_words_male',\n",
    "        'gross',\n",
    "        'mean_age_male',\n",
    "        'mean_age_female',\n",
    "        'age_lead',\n",
    "        'age_co_lead',\n",
    "        'best_k',\n",
    "        'accuracy',\n",
    "        'iteration_no'\n",
    "    ]\n",
    "\n",
    "iteration_results = pd.DataFrame(columns=results_column_names)\n",
    "\n",
    "for iteration in range(1, iterations + 1):\n",
    "        if len(feature_combinations[iteration]) >= 9: # Any number within 0 to 13 - based on the minimum # of features we want to include\n",
    "            best_k, accuracy = find_best_k_with_accuracy_cv(\n",
    "                X_train[feature_combinations[iteration]], y_train, k_iterations = 50, n_fold = 10\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                'number_words_female': 0,\n",
    "                'total_words': 0,\n",
    "                'number_of_words_lead': 0,\n",
    "                'difference_in_words_lead_and_co_lead': 0,\n",
    "                'number_of_male_actors': 0,\n",
    "                'year': 0,\n",
    "                'number_of_female_actors': 0,\n",
    "                'number_words_male': 0,\n",
    "                'gross': 0,\n",
    "                'mean_age_male': 0,\n",
    "                'mean_age_female': 0,\n",
    "                'age_lead': 0,\n",
    "                'age_co_lead': 0,\n",
    "                'best_k': best_k,\n",
    "                'accuracy': accuracy,\n",
    "                'iteration_no': iteration\n",
    "            }\n",
    "\n",
    "            for key, value in row.items():\n",
    "                if key in feature_combinations[iteration]:\n",
    "                    row[key] = 1\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            iteration_results = iteration_results.append(row, ignore_index=True)\n",
    "            iteration_results.to_csv(r'/Users/dininduseneviratne/Library/CloudStorage/OneDrive-Uppsalauniversitet/Statistical Machine Learning/project-results/results_8191.csv')\n",
    "            print(str(iteration) + \" OUT OF \" + str(iterations) + \" ITERATIONS COMPLETED - \" + str(iteration*100/iterations) + \"%\")\n",
    "\n",
    "        else: \n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c15b10ab1573516f5b110d1ec5a6cdd301921bf6293fc77ccc4498a93ca5d7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
